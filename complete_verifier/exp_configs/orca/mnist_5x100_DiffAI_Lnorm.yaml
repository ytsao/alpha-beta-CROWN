general:
  device: cpu  # Select device to run verifier, cpu or cuda (GPU).
  seed: 100  # Random seed.
  conv_mode: patches  # Convolution mode during bound propagation: "patches" mode (default) is very efficient, but may not support all architecture; "matrix" mode is slow but supports all architectures.
  deterministic: false  # Run code in CUDA deterministic mode, which has slower performance but better reproducibility.
  double_fp: false  # Use double precision floating point. GPUs with good double precision support are preferable (NVIDIA P100, V100, A100, H100; AMD Radeon Instinc MI50, MI100).
  loss_reduction_func: sum  # When batch size is not 1, this reduction function is applied to reduce the bounds into a scalar (options are "sum" and "min").
  sparse_alpha: true  # Enable/disable sparse alpha.
  sparse_interm: true  # Enable/disable sparse intermediate bounds.
  save_adv_example: false  # Save returned adversarial example in file.
  eval_adv_example: false  # Whether to validate the saved adversarial example.
  show_adv_example: false  # Print the adversarial example.
  precompile_jit: false  # Precompile jit kernels to speed up after jit-wrapped functions, but will cost extra time at the beginning.
  complete_verifier: skip # Complete verification verifier. "bab": branch and bound with beta-CROWN or GCP-CROWN; "mip": mixed integer programming (MIP) formulation; "bab-refine": branch and bound with intermediate layer bounds computed by MIP; "Customized": customized verifier, need to specially def by user.
  enable_incomplete_verification: true  # Enable/Disable initial alpha-CROWN incomplete verification (disable this can save GPU memory).
  csv_name: null  # Name of .csv file containing a list of properties to verify (VNN-COMP specific).
  results_file: out.txt  # Path to results file.
  root_path: ''  # Root path of the specification folder if using vnnlib.
  deterministic_opt: false  # Try to ensure that the bound parameters always match with the optimized bounds.
  graph_optimizer: 'Customized("custom_graph_optimizer", "default_optimizer")'  # BoundedModule model graph optimizer function name. For examples of customized graph optimizer, please see the config files for the gtrsb benchmark in VNN-COMP 2023.
  buffer_has_batchdim: false  # In most cases, the shape of buffers in an ONNX graph do not have a batch dimension. Enabling this option will help load models with a buffer object that has a batch dimension. In most case this can be inferred in the verifier automatically, and this option is not needed.
  save_output: false  # Save output for test.
  output_file: out.pkl  # Path to the output file.
  return_optimized_model: false  # Return the model with optimized bounds after incomplete verification is done.
model:
  name: ORCA_mnist_7x200_best  # Model name. Will be evaluated as a python statement.
  path: models/orca/7x200_best.pth  # Load pretrained model from this specified path.
  onnx_path: null  # Path to .onnx model file.
  onnx_path_prefix: ''  # Add a prefix to .onnx model path to correct malformed csv files.
  cache_onnx_conversion: false  # Cache the model converted from ONNX.
  onnx_quirks: null  # Load onnx model with quirks to workaround onnx model issue. This string will be passed to onnx2pytorch as the 'quirks' argument, and it is typically a literal of a python dict, e.g., "{'Reshape': {'fix_batch_size: True'}}".
  input_shape: [-1,1,28,28]  # Specified input shape of the model.Usually the shape can be automatically determined from dataset or onnx model, but some onnx models may have an incompatible shape (without batch dim). You can specify shape explicitly here.The shape should be (-1, input_shape) like (-1, 3, 32, 32) and -1 indicates the batch dim.
  onnx_loader: default_onnx_and_vnnlib_loader  # ONNX model loader function name. Can be the Customized() primitive; for examples of customized model loaders, please see the config files for the marabou-cifar10 benchmark in VNN-COMP 2021 and the Carvana benchmark in VNN-COMP 2022.
  onnx_optimization_flags: none  # Onnx graph optimization config.
  onnx_vnnlib_joint_optimization_flags: none  # Joint optimization that changes both onnx model and vnnlib.
  check_optmized: false  # Check the optimized onnx file instead the original one when converting to pytorch. This is used when input shape is changed during optimization.
  flatten_final_output: false  # Manually add a flatten layer at the end of the model.
  optimize_graph: null  # Specify a custom function for optimizing the graph on the BoundedModule.
  with_jacobian: false  # Indicate that the model contains JacobianOP.
data:
  start: 0  # Start from the i-th property in specified dataset.
  end: 10000  # End with the (i-1)-th property in the dataset.
  select_instance: null  # Select a list of instances to verify.
  num_outputs: 10  # Number of classes for classification problem.
  mean: 0.0  # Mean vector used in data preprocessing.
  std: 1.0  # Std vector used in data preprocessing.
  pkl_path: null  # Load properties to verify from a .pkl file (only used for oval20 dataset).
  dataset: MNIST  # Dataset name (only if not using specifications from a .csv file). Dataset must be defined in utils.py. For customized data, checkout custom/custom_model_data.py.
  data_idx_file: null  # A text file with a list of example IDs to run.
specification:
  type: lp  # Type of verification specification. "lp" = L_p norm, "box" = element-wise lower and upper bound provided by dataloader.
  robustness_type: verified-acc  # For robustness verification: verify against all labels ("verified-acc" mode), or just the runnerup labels ("runnerup" mode), or using a specified label in dataset ("speicify-target" mode, only used for oval20). Not used when a VNNLIB spec is used.
  norm: .inf  # Lp-norm for epsilon perturbation in robustness verification (1, 2, inf).
  epsilon: 0.1  # Set perturbation size (Lp norm). If not set, a default value may be used based on dataset loader.
  epsilon_min: 0.1  # Set an optional minimum perturbation size (Lp norm).
  vnnlib_path: null  # Path to .vnnlib specification file. Will override any Lp/robustness verification arguments.
  vnnlib_path_prefix: ''  # Add a prefix to .vnnlib specs path to correct malformed csv files.
  rhs_offset: null  # Adding an offset to RHS.
solver:
  batch_size: 64  # Batch size in bound solver (number of parallel splits).
  auto_enlarge_batch_size: false  # Automatically increase batch size based on --batch_size in bab if current VRAM usage < 45%%, only support input_split.
  min_batch_size_ratio: 0.1  # The minimum batch size ratio in each iteration (splitting multiple layers if the number of domains is smaller than min_batch_size_ratio * batch_size).
  use_float64_in_last_iteration: false  # Use double fp (float64) at the last iteration in alpha/beta CROWN.
  early_stop_patience: 10  # Number of iterations that we will start considering early stop if tracking no improvement.
  start_save_best: 0.5  # Start to save best optimized bounds when i > int(iteration*start_save_best). Early iterations are skipped for better efficiency.
  bound_prop_method: alpha-crown  # Bound propagation method used for incomplete verification and input split based branch and bound.
  init_bound_prop_method: same  # Bound propagation method used for the initial bound in input split based branch and bound. If "same" is specified, then it will use the same method as "bound_prop_method".
  prune_after_crown: false  # After CROWN pass, prune verified labels before starting the alpha-CROWN pass.
  optimize_disjuncts_separately: false  # If set, each neuron computes seperate bounds for each disjunct. If set, do not set prune_after_crown=True.
  crown:
    batch_size: 1000000000  # Batch size in batched CROWN.
    max_crown_size: 1000000000  # Max output size in CROWN (when there are too many output neurons, only part of them will be bounded by CROWN).
    relu_option: adaptive  # Options for specifying the the way to initialize CROWN bounds for ReLU function.
  alpha-crown:
    alpha: true  # Disable/Enable alpha crown.
    lr_alpha: 0.1  # Learning rate for the optimizable parameter alpha in alpha-CROWN bound.
    iteration: 100  # Number of iterations for alpha-CROWN incomplete verifier.
    share_alphas: false  # Share some alpha variables to save memory at the cost of slightly looser bounds.
    lr_decay: 0.98  # Learning rate decay factor during alpha-CROWN optimization. Need to use a larger value like 0.99 or 0.995 when you increase the number of iterations.
    full_conv_alpha: true  # Enable/disable the use of independent alpha for conv layers.
    max_coeff_mul: .inf  # Maximum coefficient value in the optimizable parameters for BoundMul.
    matmul_share_alphas: false  # Check alpha sharing for matmul.
    disable_optimization: []  # A list of the names of operators which have bound optimization disabled.
  invprop:
    apply_output_constraints_to: []  # Includes the output constraint in the optimization of linear layers. Can be a comma seperated list of layer types (e.g. "BoundLinear"), layer names (e.g. "/input.7") or "all". This will disable patch mode for listed conv layers. When set, --optimize_disjuncts_separately must be set, too, if the safety property uses a disjunction.
    tighten_input_bounds: false  # Tighten input bounds using output constraints. If set, --apply_output_constraints_to should contain "BoundInput" or the corresponding layer name.
    best_of_oc_and_no_oc: false  # Computes bounds of each layer both with and without output constraints. The best of both is used. Increases the runtime, but may improve the bounds.
    directly_optimize: []  # A list of layer names whose bounds should be directly optimized for. IBP is disabled for these layers. Should only be needed for backward verification (approximation of input bounds) where the first linear layer defines the cs and should be optimized.
    oc_lr: 0.1  # The learning rate for the dualized output constraints.
    share_gammas: false  # Shares gammas across neurons in the optimized layer.
  beta-crown:
    lr_alpha: 0.01  # Learning rate for optimizing alpha during branch and bound.
    lr_beta: 0.05  # Learning rate for optimizing beta during branch and bound.
    lr_decay: 0.98  # Learning rate decay factor during beta-CROWN optimization. Need to use a larger value like 0.99 or 0.995 when you increase the number of iterations.
    optimizer: adam  # Optimizer used for alpha and beta optimization.
    iteration: 50  # Number of iteration for optimizing alpha and beta during branch and bound.
    enable_opt_interm_bounds: false  # Enable optimizing intermediate bounds for beta-CROWN, only used when mip refine for now.
    all_node_split_LP: false  # When all nodes are split during Bab but not verified, using LP to check.
  forward:
    refine: false  # Refine forward bound with CROWN for unstable neurons.
    dynamic: false  # Use dynamic forward bound propagation where new input variables may be dynamically introduced for nonlinearities.
    max_dim: 10000  # Maximum input dimension for forward bounds in a batch.
    reset_threshold: 1.0  # Reset the start layer if timeout neurons are above the threshold.
  multi_class:
    label_batch_size: 32  # Maximum target labels to handle in alpha-CROWN. Cannot be too large due to GPU memory limit.
    skip_with_refined_bound: true  # By default we skip the second alpha-CROWN execution if all alphas are already initialized. Setting this to avoid this feature.
  mip:
    parallel_solvers: null  # Number of multi-processes for mip solver. Each process computes a mip bound for an intermediate neuron. Default (None) is to auto detect the number of CPU cores (note that each process may use multiple threads, see the next option).
    solver_threads: 1  # Number of threads for echo mip solver process (default is to use 1 thread for each solver process).
    refine_neuron_timeout: 15  # MIP timeout threshold for improving each intermediate layer bound (in seconds).
    refine_neuron_time_percentage: 0.8  # Percentage (x100%%) of time used for improving all intermediate layer bounds using mip. Default to be 0.8*timeout.
    early_stop: true  # Enable/disable early stop when finding a positive lower bound or a adversarial example during MIP.
    adv_warmup: true  # Disable using PGD adv as MIP refinement warmup starts.
    mip_solver: gurobi  # MLP/LP solver package (SCIP support is experimental).
